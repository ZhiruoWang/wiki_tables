{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.experimental.preprocessing'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-de23a98cb67e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepeatVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     raise ImportError(\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import Model, Input\n",
    "from keras.layers import Embedding, Dropout, Bidirectional, LSTM, K, Lambda, Dense, RepeatVector, TimeDistributed, merge, Activation, Flatten, Permute, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.core import *\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, merge, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import random\n",
    "\n",
    "# from DataLoader import DataLoader\n",
    "from table import WikiTable\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Compute P/R/F1 from the confusion matrix. '''\n",
    "def evaluation_metrics_report(mat, labels_, method_, epochs=10):\n",
    "    num_classes = len(mat)\n",
    "    scores = dict()\n",
    "    avg_p = []\n",
    "    avg_r = []\n",
    "    avg_f1 = []\n",
    "    for i in range(0, num_classes):\n",
    "        p = mat[i,i] / float(sum(mat[:,i]))\n",
    "        r = mat[i,i] / float(sum(mat[i,:]))\n",
    "        f1 = 2 * (p * r) / (p + r)\n",
    "        scores[i] = (p, r, f1)\n",
    "        avg_p.append(p)\n",
    "        avg_r.append(r)\n",
    "        avg_f1.append(f1)\n",
    "    outstr = 'Evaluation results for ' + method_ + ' Epochs: ' + str(epochs) + '\\n'\n",
    "    for key in scores:\n",
    "        label = labels_[key]\n",
    "        val_1 = scores[key][0]\n",
    "        val_2 = scores[key][1]\n",
    "        val_3 = scores[key][2]\n",
    "        outstr += ('%s\\tP=%.3f\\tR=%.3f\\tF1=%.3f\\n' % (label, val_1, val_2, val_3))\n",
    "    avg_p_score = sum(avg_p) / len(avg_p)\n",
    "    avg_r_score = sum(avg_r) / len(avg_r)\n",
    "    avg_f1_score = sum(avg_f1) / len(avg_f1)\n",
    "    outstr += 'AVG\\tAvg-P=%.3f\\tAvg-R=%.3f\\tAvg-F1=%.3f\\n' % (avg_p_score, avg_r_score, avg_f1_score)\n",
    "    return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''zr test'''\n",
    "mat = np.array([[1,2,3], [2,3,2], [3,4,4]])\n",
    "labels = [\"a\", \"b\", \"c\"]\n",
    "method = \"NoMethod\"\n",
    "print(evaluation_metrics_report(mat, labels, method))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM column-by-column with attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_H_n(X):\n",
    "    ans = X[:, -1, :]  # get last element from time dim\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_Y(X, xmaxlen):\n",
    "    return X[:, :xmaxlen, :]  # get first xmaxlen elem from time dim\n",
    "\n",
    "\n",
    "def get_R(X):\n",
    "    Y, alpha = X[0], X[1]\n",
    "    ans = K.batch_dot(Y, alpha)\n",
    "    return ans\n",
    "\n",
    "'''LSTM baseline where the columns are represented by their title, and LCA category. '''\n",
    "\n",
    "def build_lstm_baseline_w2v(w2v_size, w2v_dim, word2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    drop_out = Dropout(0.3, name='dropout')(e)\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    lstm = LSTM(lstm_units, return_sequences=True)(drop_out)\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.2)(lstm)\n",
    "    flatten = Flatten()(drop_out)\n",
    "    out = Dense(3, activation='softmax')(flatten)\n",
    "\n",
    "    model = Model(input=main_input, output=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "'''LSTM baseline where the columns are represented by their title, and LCA category. '''\n",
    "def build_lstm_baseline_w2v_lca(w2v_size, w2v_dim, word2vec_matrix, n2v_size, n2v_dim, node2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    subject_input = Input(shape=(N,), dtype='int32', name='subject_input')\n",
    "    e2 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(subject_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    e4 = merge([e, e2], mode='sum')\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    drop_out = Dropout(0.3, name='dropout')(e4)\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    lstm = LSTM(lstm_units, return_sequences=True)(drop_out)\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.2)(lstm)\n",
    "    flatten = Flatten()(drop_out)\n",
    "    out = Dense(3, activation='softmax')(flatten)\n",
    "\n",
    "    model = Model(input=[main_input, subject_input], output=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''LSTM baseline where the columns are represented by their title, and LCA category. '''\n",
    "def build_lstm_baseline_w2v_lca_val(w2v_size, w2v_dim, word2vec_matrix, n2v_size, n2v_dim, node2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    subject_input = Input(shape=(N,), dtype='int32', name='subject_input')\n",
    "    e2 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(subject_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    value_input = Input(shape=(N,), dtype='int32', name='value_input')\n",
    "    e3 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(value_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    e4 = merge([e, e2, e3], mode='sum')\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    drop_out = Dropout(0.3, name='dropout')(e4)\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    lstm = LSTM(lstm_units, return_sequences=True)(drop_out)\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.2)(lstm)\n",
    "    flatten = Flatten()(drop_out)\n",
    "    out = Dense(3, activation='softmax')(flatten)\n",
    "\n",
    "    model = Model(input=[main_input, subject_input, value_input], output=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''BiLSTM baseline where the columns are represented by their title, and LCA category. '''\n",
    "def build_bilstm_baseline_w2v_lca(w2v_size, w2v_dim, word2vec_matrix, n2v_size, n2v_dim, node2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    subject_input = Input(shape=(N,), dtype='int32', name='subject_input')\n",
    "    e2 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(subject_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    e4 = merge([e, e2], mode='sum')\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    drop_out = Dropout(0.3, name='dropout')(e4)\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(drop_out)\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.2)(bilstm)\n",
    "    flatten = Flatten()(drop_out)\n",
    "    out = Dense(3, activation='softmax')(flatten)\n",
    "\n",
    "    model = Model(input=[main_input, subject_input], output=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "'''BiLSTM baseline where the columns are represented by their title. '''\n",
    "def build_bilstm_baseline_w2v(w2v_size, w2v_dim, word2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    drop_out = Dropout(0.3, name='dropout')(e)\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(drop_out)\n",
    "\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.2)(bilstm)\n",
    "    flatten = Flatten()(drop_out)\n",
    "    out = Dense(3, activation='softmax')(flatten)\n",
    "    output = out\n",
    "\n",
    "    model = Model(input=main_input, output=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''BiLSTM baseline where the columns are represented by their title, LCA category, and column values. '''\n",
    "def build_bilstm_baseline_w2v_lca_val(w2v_size, w2v_dim, word2vec_matrix, n2v_size, n2v_dim, node2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    subject_input = Input(shape=(N,), dtype='int32', name='subject_input')\n",
    "    e2 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(subject_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    value_input = Input(shape=(N,), dtype='int32', name='value_input')\n",
    "    e3 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(value_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    e4 = merge([e, e2, e3], mode='sum')\n",
    "    drop_out = Dropout(0.3, name='dropout')(e4)\n",
    "\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(drop_out)\n",
    "\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.2)(bilstm)\n",
    "    flatten = Flatten()(drop_out)\n",
    "    out = Dense(3, activation='softmax')(flatten)\n",
    "\n",
    "    model = Model(input=[main_input, subject_input, value_input], output=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "We build a BiLSTM model with attention, which matches two tables based on their column representation.\n",
    "The attention mechanism decides which columns have their highest match from the second candidate table\n",
    "and then uses this to compute the attention weights which later on are used for classifying the pair\n",
    "as either: 'equivalent', 'subPartOf' or 'notAligned'\n",
    "\n",
    "The model is based on the paper by Rockta schel et al. \"Reasoning about Entailment with Neural Attention\"\n",
    "\n",
    "@Credit: This is an adaptation of the implementation of the original paper by https://github.com/shyamupa/snli-entailment\n",
    "'''\n",
    "def build_bilstm_col_subject_val_model(w2v_size, w2v_dim, word2vec_matrix, n2v_size, n2v_dim, node2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = LEN    # ??\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    subject_input = Input(shape=(N,), dtype='int32', name='subject_input')\n",
    "    e2 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(subject_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    value_input = Input(shape=(N,), dtype='int32', name='value_input')\n",
    "    e3 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(value_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    e4 = merge([e, e2, e3], mode='sum')\n",
    "\n",
    "    drop_out = Dropout(0.3, name='dropout')(e4)\n",
    "\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(drop_out)\n",
    "\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.1)(bilstm)\n",
    "\n",
    "    # here we add a custom layer which connects the last input of the first table as the input of the second table\n",
    "    t2 = Lambda(get_H_n, output_shape=(k,), name='h_n')(drop_out)\n",
    "\n",
    "    # add the layer which encodes the output labels\n",
    "    Y = Lambda(get_Y, arguments={'xmaxlen': col_length}, name='Y', output_shape=(col_length, k))(drop_out)\n",
    "    # add the layer which encodes the weight parameter from the LSTMs cells and the hidden states\n",
    "    Whn = Dense(k, W_regularizer=l2(0.01), name=\"Wh_n\")(t2)\n",
    "    Whn_x_e = RepeatVector(col_length, name=\"Wh_n_x_e\")(Whn)\n",
    "\n",
    "    # add the attention layer\n",
    "    WY = TimeDistributed(Dense(2 * lstm_units, W_regularizer=l2(0.01)), name=\"WY\")(Y)\n",
    "    merged = merge([Whn_x_e, WY], name='merged', mode='sum')\n",
    "    M = Activation('tanh', name='M')(merged)\n",
    "\n",
    "    alpha_ = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_\")(M)\n",
    "    flat_alpha = Flatten(name=\"flat_alpha\")(alpha_)\n",
    "    alpha = Dense(col_length, activation='softmax', name=\"alpha\")(flat_alpha)\n",
    "\n",
    "    Y_trans = Permute((2, 1), name=\"y_trans\")(Y)  # of shape (None,300,20)\n",
    "    r_ = merge([Y_trans, alpha], output_shape=(k, 1), name=\"r_\", mode=get_R)\n",
    "    r = Reshape((k,), name=\"r\")(r_)\n",
    "\n",
    "    Wr = Dense(k, W_regularizer=l2(0.01))(r)\n",
    "    Wh = Dense(k, W_regularizer=l2(0.01))(t2)\n",
    "    merged = merge([Wr, Wh], mode='sum')\n",
    "    h_star = Activation('tanh')(merged)\n",
    "\n",
    "    out = Dense(3, activation='softmax')(h_star)\n",
    "    output = out\n",
    "\n",
    "    model = Model(input=[main_input, subject_input, value_input], output=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "We build a BiLSTM model with attention, which matches two tables based on their column representation.\n",
    "The attention mechanism decides which columns have their highest match from the second candidate table\n",
    "and then uses this to compute the attention weights which later on are used for classifying the pair\n",
    "as either: 'equivalent', 'subPartOf' or 'notAligned'\n",
    "\n",
    "The model is based on the paper by Rockta schel et al. \"Reasoning about Entailment with Neural Attention\"\n",
    "@Credit: This is an adaptation of the implementation of the original paper by https://github.com/shyamupa/snli-entailment\n",
    "'''\n",
    "def build_bilstm_col_subject_model(w2v_size, w2v_dim, word2vec_matrix, n2v_size, n2v_dim, node2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    subject_input = Input(shape=(N,), dtype='int32', name='subject_input')\n",
    "    e2 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(subject_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    e3 = merge([e, e2], mode='sum')\n",
    "\n",
    "    drop_out = Dropout(0.3, name='dropout')(e3)\n",
    "\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(drop_out)\n",
    "\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.1)(bilstm)\n",
    "\n",
    "    # here we add a custom layer which connects the last input of the first table as the input of the second table\n",
    "    t2 = Lambda(get_H_n, output_shape=(k,), name='h_n')(drop_out)\n",
    "\n",
    "    # add the layer which encodes the output labels\n",
    "    Y = Lambda(get_Y, arguments={'xmaxlen': col_length}, name='Y', output_shape=(col_length, k))(drop_out)\n",
    "    # add the layer which encodes the weight parameter from the LSTMs cells and the hidden states\n",
    "    Whn = Dense(k, W_regularizer=l2(0.01), name=\"Wh_n\")(t2)\n",
    "    Whn_x_e = RepeatVector(col_length, name=\"Wh_n_x_e\")(Whn)\n",
    "\n",
    "    # add the attention layer\n",
    "    WY = TimeDistributed(Dense(2 * lstm_units, W_regularizer=l2(0.01)), name=\"WY\")(Y)\n",
    "    merged = merge([Whn_x_e, WY], name='merged', mode='sum')\n",
    "    M = Activation('tanh', name='M')(merged)\n",
    "\n",
    "    alpha_ = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_\")(M)\n",
    "    flat_alpha = Flatten(name=\"flat_alpha\")(alpha_)\n",
    "    alpha = Dense(col_length, activation='softmax', name=\"alpha\")(flat_alpha)\n",
    "\n",
    "    Y_trans = Permute((2, 1), name=\"y_trans\")(Y)  # of shape (None,300,20)\n",
    "    r_ = merge([Y_trans, alpha], output_shape=(k, 1), name=\"r_\", mode=get_R)\n",
    "    r = Reshape((k,), name=\"r\")(r_)\n",
    "\n",
    "    Wr = Dense(k, W_regularizer=l2(0.01))(r)\n",
    "    Wh = Dense(k, W_regularizer=l2(0.01))(t2)\n",
    "    merged = merge([Wr, Wh], mode='sum')\n",
    "    h_star = Activation('tanh')(merged)\n",
    "\n",
    "    out = Dense(3, activation='softmax')(h_star)\n",
    "    output = out\n",
    "\n",
    "    model = Model(input=[main_input, subject_input], output=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "We build a BiLSTM model with attention, which matches two tables based on their column representation.\n",
    "The attention mechanism decides which columns have their highest match from the second candidate table\n",
    "and then uses this to compute the attention weights which later on are used for classifying the pair\n",
    "as either: 'equivalent', 'subPartOf' or 'notAligned'\n",
    "\n",
    "The model is based on the paper by Rockta schel et al. \"Reasoning about Entailment with Neural Attention\"\n",
    "@Credit: This is an adaptation of the implementation of the original paper by https://github.com/shyamupa/snli-entailment\n",
    "'''\n",
    "def build_bilstm_col_model(vocab_size, w2v_dim, embedding_matrix, lstm_units=100, col_length=20):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(vocab_size, w2v_dim, weights=[embedding_matrix], input_length=N, trainable=False)(main_input)\n",
    "    drop_out = Dropout(0.1, name='droput')(e)\n",
    "\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(drop_out)\n",
    "\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.1)(bilstm)\n",
    "\n",
    "    # here we add a custom layer which connects the last input of the first table as the input of the second table\n",
    "    t2 = Lambda(get_H_n, output_shape=(k,), name='h_n')(drop_out)\n",
    "\n",
    "    # add the layer which encodes the output labels\n",
    "    Y = Lambda(get_Y, arguments={'xmaxlen': col_length}, name='Y', output_shape=(col_length, k))(drop_out)\n",
    "    # add the layer which encodes the weight parameter from the LSTMs cells and the hidden states\n",
    "    Whn = Dense(2 * lstm_units, W_regularizer=l2(0.01), name=\"Wh_n\")(t2)\n",
    "    Whn_x_e = RepeatVector(col_length, name=\"Wh_n_x_e\")(Whn)\n",
    "\n",
    "    # add the attention layer\n",
    "    WY = TimeDistributed(Dense(2 * lstm_units, W_regularizer=l2(0.01)), name=\"WY\")(Y)\n",
    "    merged = merge([Whn_x_e, WY], name='merged', mode='sum')\n",
    "    M = Activation('sigmoid', name='M')(merged)\n",
    "\n",
    "    alpha_ = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_\")(M)\n",
    "    flat_alpha = Flatten(name=\"flat_alpha\")(alpha_)\n",
    "    alpha = Dense(col_length, activation='softmax', name=\"alpha\")(flat_alpha)\n",
    "\n",
    "    Y_trans = Permute((2, 1), name=\"y_trans\")(Y)  # of shape (None,300,20)\n",
    "    r_ = merge([Y_trans, alpha], output_shape=(2 * lstm_units, 1), name=\"r_\", mode=get_R)\n",
    "    r = Reshape((k,), name=\"r\")(r_)\n",
    "\n",
    "    Wr = Dense(k, W_regularizer=l2(0.01))(r)\n",
    "    Wh = Dense(k, W_regularizer=l2(0.01))(t2)\n",
    "    merged = merge([Wr, Wh], mode='sum')\n",
    "    h_star = Activation('sigmoid')(merged)\n",
    "\n",
    "    out = Dense(3, activation='softmax')(h_star)\n",
    "    output = out\n",
    "\n",
    "    model = Model(input=[main_input], output=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we store the pre-loaded embeddings so that we do not load that every time we do changes in the class.\n",
    "word2vec = None\n",
    "node2vec = None\n",
    "tables = dict()\n",
    "entity_cats = dict()\n",
    "cat_tax = dict()\n",
    "cat_level = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_w2v = None\n",
    "emb_n2v = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verse_emb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nltk.corpus",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c6209907d739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named nltk.corpus"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "from table import WikiTable\n",
    "import re, os, sys\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self):\n",
    "        # the stop words, which we will use to skip stop words from the column titles\n",
    "        self.stops = set(stopwords.words('english'))\n",
    "        self.word2vec_path = ''\n",
    "        self.node2vec_path = ''\n",
    "        self.cat_taxonomy_path = ''\n",
    "        self.entity_category_path = ''\n",
    "        self.table_data_path = ''\n",
    "        self.table_data_labels = ''\n",
    "        self.out_dir = ''\n",
    "\n",
    "        # data structures that which will hold the necessary data for training the models\n",
    "        self.entity_cats = dict()\n",
    "        self.word2vec = None\n",
    "        self.node2vec = None\n",
    "        self.vocab_n2v = dict()\n",
    "        self.vocab_w2v = dict()\n",
    "        self.DELIM = 0\n",
    "        self.table_pairs = []\n",
    "        self.tables = dict()\n",
    "        self.table_rep = dict()\n",
    "        self.eval_pairs = []\n",
    "        self.cat_tax = dict()\n",
    "        self.cat_parents = dict()\n",
    "        self.cat_level = dict()\n",
    "\n",
    "\n",
    "    '''Load the word2vec and the node2vec embeddings. '''\n",
    "    def load_embeddings(self):\n",
    "        self.word2vec = KeyedVectors.load_word2vec_format(self.word2vec_path, binary=False)\n",
    "        self.node2vec = KeyedVectors.load_word2vec_format(self.node2vec_path, binary=False)\n",
    "\n",
    "    def construct_vocab(self):\n",
    "        self.vocab_w2v = {key: idx + 2 for idx, key in enumerate(self.word2vec.vocab)}\n",
    "\n",
    "        # keep zero for the unknown words\n",
    "        self.vocab_w2v['UNK'] = 0\n",
    "        self.vocab_w2v['COL_TOKEN_SPLIT'] = 1\n",
    "\n",
    "        self.vocab_n2v = {key: idx + 2 for idx, key in enumerate(self.node2vec.vocab)}\n",
    "        self.vocab_n2v['UNK'] = 0\n",
    "        self.vocab_n2v['COL_TOKEN_SPLIT'] = 1\n",
    "\n",
    "        # the delimiter which we use to stitch tables together\n",
    "        self.DELIM_N2V = [len(self.vocab_n2v)]\n",
    "        self.DELIM_W2V = [len(self.vocab_w2v)]\n",
    "\n",
    "    '''Load the evaluation dataset. '''\n",
    "    def load_evaluation_data(self):\n",
    "        for table_pair in self.table_pairs:\n",
    "            table_pair_dict = dict()\n",
    "            table_pair_dict['col_name'] = []\n",
    "            table_pair_dict['col_values'] = []\n",
    "            table_pair_dict['col_subject'] = []\n",
    "\n",
    "            table_a = self.table_rep[table_pair[0]]\n",
    "            table_b = self.table_rep[table_pair[1]]\n",
    "\n",
    "            # generate the different representations\n",
    "            label = table_pair[2]\n",
    "\n",
    "            # concatenate the features for the different tables\n",
    "            ep_col_name = np.concatenate((np.concatenate(table_a['col_name'].values()), self.DELIM_W2V, np.concatenate(table_b['col_name'].values())), axis=0)\n",
    "            ep_col_values = np.concatenate((np.concatenate(table_a['col_values'].values()), self.DELIM_N2V, np.concatenate(table_b['col_values'].values())), axis=0)\n",
    "            ep_col_subject = np.concatenate((np.concatenate(table_a['col_subject'].values()), self.DELIM_N2V, np.concatenate(table_b['col_subject'].values())), axis=0)\n",
    "\n",
    "            # ep_col_name = np.concatenate((np.concatenate(table_a['col_name'].values(), axis=0), self.DELIM_W2V, np.concatenate(table_b['col_name'].values(), axis=0)), axis=0)\n",
    "            # ep_col_values = np.concatenate((np.concatenate(table_a['col_values'].values(), axis=0), self.DELIM_N2V, np.concatenate(table_b['col_values'].values(), axis=0)), axis=0)\n",
    "            # ep_col_subject = np.concatenate((np.concatenate(table_a['col_subject'].values(), axis=0), self.DELIM_N2V, np.concatenate(table_b['col_subject'].values(), axis=0)), axis=0)\n",
    "\n",
    "            table_pair_dict['col_name'].append(ep_col_name)\n",
    "            table_pair_dict['col_values'].append(ep_col_values)\n",
    "            table_pair_dict['col_subject'].append(ep_col_subject)\n",
    "\n",
    "            table_pair_dict['table_a'] = table_pair[0]\n",
    "            table_pair_dict['table_b'] = table_pair[1]\n",
    "            table_pair_dict['label'] = label\n",
    "            self.eval_pairs.append(table_pair_dict)\n",
    "\n",
    "\n",
    "    '''Load the table pairs for alignment with their table data values and their corresponding label. '''\n",
    "    def load_alignment_pairs(self):\n",
    "        # load the table data only for these entities and the table alignment pairs\n",
    "        self.table_pairs = []\n",
    "\n",
    "        eval_data = DataFrame.from_csv(self.table_data_labels, sep='\\\\t', index_col=None)\n",
    "        for row_idx, row in eval_data.iterrows():\n",
    "            # the label for the table alignment pair\n",
    "            label = row['label']\n",
    "            # add the entities in the entity index\n",
    "\n",
    "            # add the table pairs as a tuple with the label\n",
    "            tbl_id_a = int(row['tbl_id_a'])\n",
    "            tbl_id_b = int(row['tbl_id_b'])\n",
    "            self.table_pairs.append((tbl_id_a, tbl_id_b, label))\n",
    "\n",
    "        # load the table data\n",
    "        self.load_tables()\n",
    "\n",
    "\n",
    "    '''Load the table data for a set of entities of interest. '''\n",
    "    def load_tables(self):\n",
    "        # read the table data and take only the tables for the entities in the entity index.\n",
    "        fin = gzip.open(self.table_data_path, 'rt')\n",
    "\n",
    "        # return the tables as a dict with the table id as an index.\n",
    "        self.tables = dict()\n",
    "\n",
    "        for line in fin:\n",
    "            if len(line.strip()) == 0:\n",
    "                continue\n",
    "            tbl_json = json.loads(line)\n",
    "            entity = tbl_json['entity']\n",
    "            sections = tbl_json['sections']\n",
    "            for section in sections:\n",
    "                tables_json = section['tables']\n",
    "                for table in tables_json:\n",
    "                    tbl = WikiTable()\n",
    "                    tbl.load_json(json.dumps(table), entity, section, int(table['id']), col_meta_parse=True)\n",
    "                    tbl.markup = table['markup']\n",
    "                    self.tables[tbl.table_id] = tbl\n",
    "\n",
    "        '''\n",
    "        Constructs the evaluation data which we will use to train our DL model. \n",
    "        In this case we will represent the data in the following ways:\n",
    "            1)  The simplest form of the data representation is in terms of the column names.\n",
    "            2)  We augment the representation with the entities or values present in a column, \n",
    "                in case the values do not link to entities or are not textual, \n",
    "                then we will represent the column data with a UNK vector.\n",
    "            3)  Finally, in the case where (2) reflects entities, we will additionally represent\n",
    "                the data with the corresponding column label (i.e., the LCA category of entities)\n",
    "        '''\n",
    "        def construct_eval_data(self):\n",
    "            self.table_rep.clear()\n",
    "            # generate the appropriate table representation that we can use for the deep learning models.\n",
    "            for table_id in self.tables:\n",
    "                table = self.tables[table_id]\n",
    "                sub_table_rep = dict()\n",
    "\n",
    "                # generate the column representation, for the columns for which we do not have a word we assign UNK\n",
    "                col_names_rep = dict()\n",
    "                col_val_rep = dict()\n",
    "                col_subj_rep = dict()\n",
    "\n",
    "                for col_idx, column in enumerate(table.column_meta_data):\n",
    "                    col_subj_rep[col_idx] = []\n",
    "                    col_val_rep[col_idx] = []\n",
    "                    col_names_rep[col_idx] = []\n",
    "\n",
    "                    # generate the word embedding for the column name\n",
    "                    col_names_rep[col_idx] = self.column_title_to_idx(column, self.vocab_w2v)\n",
    "\n",
    "                    # generate the column representation based on the entities\n",
    "                    col_values = table.column_meta_data[column]\n",
    "\n",
    "                    # take the subset of values which exist in our entity-category index\n",
    "                    sub_vals = []\n",
    "                    for val in col_values:\n",
    "                        val_label = re.sub(' ', '_', val)\n",
    "                        if val_label in self.vocab_n2v:\n",
    "                            sub_vals.append(val_label)\n",
    "                    for val in sub_vals:\n",
    "                        col_val_rep[col_idx].append(self.vocab_n2v[val])\n",
    "                    col_val_rep[col_idx].append(self.vocab_n2v['COL_TOKEN_SPLIT'])\n",
    "\n",
    "                    # get the lca category for the values in this column (in case they represent entities)\n",
    "                    if len(col_values) != 0:\n",
    "                        lca_cats = self.find_lca_category(col_values)\n",
    "\n",
    "                        if lca_cats is not None:\n",
    "                            for cat in lca_cats: \n",
    "                                cat = 'Category:' + re.sub(' ', '_', cat)\n",
    "                                if cat not in self.vocab_n2v:\n",
    "                                    continue\n",
    "                                col_subj_rep[col_idx].append(self.vocab_n2v[cat])\n",
    "                        else:\n",
    "                            col_subj_rep[col_idx].append(0)\n",
    "\n",
    "                    # distinguish between the column representations\n",
    "                    col_subj_rep[col_idx].append(self.vocab_n2v['COL_TOKEN_SPLIT'])\n",
    "                    col_names_rep[col_idx].append(self.vocab_w2v['COL_TOKEN_SPLIT'])\n",
    "                    col_val_rep[col_idx].append(self.vocab_n2v['COL_TOKEN_SPLIT'])\n",
    "                sub_table_rep['col_name'] = col_names_rep\n",
    "                sub_table_rep['col_values'] = col_val_rep\n",
    "                sub_table_rep['col_subject'] = col_subj_rep\n",
    "                self.table_rep[table_id] = sub_table_rep\n",
    "\n",
    "\n",
    "        '''\n",
    "        For a given set of seed entities return their common LCA category. This is in a way representing the subject or class of the given entities.\n",
    "        '''\n",
    "        def find_lca_category(self, entities):\n",
    "            for entity in entities:\n",
    "                if entity not in self.entity_cats:\n",
    "                    continue\n",
    "                if entity not in self.cat_parents:\n",
    "                    self.cat_parents[entity] = []\n",
    "                #load all the categories and the parent categories for an entity\n",
    "                for cat in self.entity_cats[entity]:\n",
    "                    self.load_cat_parents(cat, self.cat_parents[entity])\n",
    "\n",
    "            # find the common categories\n",
    "            entity = entities[0]\n",
    "            common_cats = set()\n",
    "            index = 0\n",
    "            for entity in entities:\n",
    "                if entity not in self.cat_parents:\n",
    "                    continue\n",
    "                if index == 0:\n",
    "                    common_cats = set(self.cat_parents[entity])\n",
    "                    index += 1\n",
    "                else:\n",
    "                    common_cats.intersection(self.cat_parents[entity])\n",
    "\n",
    "            # get the lowest matching category\n",
    "            if len(common_cats) != 0:\n",
    "                common_cat_level = [self.cat_level[cat] for cat in common_cats if cat in self.cat_level]\n",
    "                if len(common_cat_level) != 0:\n",
    "                    max_level = max(common_cat_level)\n",
    "                    return [cat for cat in common_cats if self.cat_level[cat] == max_level]\n",
    "\n",
    "            return None\n",
    "\n",
    "\n",
    "        '''Since a column name might have different words, we split and aggregate the word vectors from the resulting words. '''\n",
    "        def column_title_tb_idx(self, column, vocab):\n",
    "            col_words = column.lower().split()\n",
    "            col_rep = []\n",
    "\n",
    "            if len(col_words) == 1:\n",
    "                if col_words[0] in vocab and col_words[0] not in self.stops:\n",
    "                    col_rep.append(vocab[col_words[0]])\n",
    "                else:\n",
    "                    col_rep.append(vocab['UNK'])\n",
    "            else:\n",
    "                for col in col_words:\n",
    "                    if col in vocab and col not in self.stops:\n",
    "                        col_rep.append(vocab[col])\n",
    "                    else:\n",
    "                        col_rep.append(vocab['UNK'])\n",
    "            col_rep.append(vocab['COL_TOKEN_SPLIT'])\n",
    "            return col_rep\n",
    "\n",
    "\n",
    "        '''Load the parents of a category up to the root. '''\n",
    "        def load_cat_parents(self, cat, parents):\n",
    "            if cat in self.cat_tax and cat not in parents:\n",
    "                sub_parents = self.cat_tax[cat]\n",
    "                parents.append(cat)\n",
    "                for parent in sub_parents:\n",
    "                    self.load_cat_parents(parent, parents)\n",
    "\n",
    "\n",
    "        '''Load the category taxonomy where each node has contains its parents. '''\n",
    "        def load_flat_cat_tax(self):\n",
    "            for line in gzip.open(self.cat_taxonomy_path, 'rt'):\n",
    "                data = line.strip().split('\\t')\n",
    "\n",
    "                parent_cat = data[0]\n",
    "                child_cat = data[2]\n",
    "\n",
    "                if parent_cat not in self.cat_level:\n",
    "                    self.cat_level[parent_cat] = int(data[1])\n",
    "                if child_cat not in self.cat_level:\n",
    "                    self.cat_level[child_cat] = int(data[3])\n",
    "\n",
    "                if child_cat not in self.cat_tax:\n",
    "                    self.cat_tax[child_cat] = []\n",
    "                self.cat_tax[child_cat].append(parent_cat)\n",
    "\n",
    "\n",
    "        '''Loads the entity categories. '''\n",
    "        def load_entity_cats(self):\n",
    "            for line in gzip.open(self.entity_category_path, 'rt'):\n",
    "                data = line.strip().split('\\t')\n",
    "                if len(data) != 2:\n",
    "                    continue\n",
    "                if data[0] not in self.entity_cats:\n",
    "                    self.entity_cats[data[0]] = []\n",
    "                self.entity_cats[data[0]].append(data[1])\n",
    "\n",
    "\n",
    "        '''The word2vec embedding matrix. '''\n",
    "        def get_word2vec_matrix(self, emb_dim):\n",
    "            # This will be the embedding matrix\n",
    "            embeddings = 1 * np.random.randn(len(self.vocab_w2v) + 2, emb_dim)\n",
    "            embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "            # Build the embedding matrix\n",
    "            for word, index in self.vocab_w2v.items():\n",
    "                if word in self.word2vec.vocab:\n",
    "                    embeddings[index] = self.word2vec.word_vec(word)[:emb_dim]\n",
    "\n",
    "            # del self.word2vec\n",
    "            return embeddings\n",
    "\n",
    "\n",
    "        '''The node2vec embedding matrix. '''\n",
    "        def get_node2vec_matrix(self, emb_dim):\n",
    "            # This will be the embedding matrix\n",
    "            embeddings = 1 * np.random.randn(len(self.vocab_n2v) + 2, emb_dim)\n",
    "            embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "            # Build the embedding matrix\n",
    "            for word, index in self.vocab_n2v.items():\n",
    "                if word in self.node2vec.vocab:\n",
    "                    embeddings[index] = self.node2vec.word_vec(word)[:emb_dim]\n",
    "            # del self.node2vec\n",
    "            return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can download the data from http://l3s.de/~fetahu/wiki_tables/data/\n",
    "base_dir = '/home/fetahu/wiki_tables/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8e4657d4d4a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create the class DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "# create the class DataLoader\n",
    "loader = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2236af9b378d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat_taxonomy_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'category_data/flat_cat_taxonomy.tsv.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/table_data/html_data/structured_html_table_data_ground_truth.json.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'embeddings/glove.6B.300d.emb.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode2vec_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'embeddings/category_entity_label_node2vec.emb.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_category_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'category_data/article_cats_201708.tsv.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "loader.cat_taxonomy_path = base_dir + 'category_data/flat_cat_taxonomy.tsv.gz'\n",
    "loader.table_data_path = '../data/table_data/html_data/structured_html_table_data_ground_truth.json.gz'\n",
    "loader.word2vec_path = base_dir + 'embeddings/glove.6B.300d.emb.gz'\n",
    "loader.node2vec_path = base_dir + 'embeddings/category_entity_label_node2vec.emb.gz'\n",
    "loader.entity_category_path = base_dir + 'category_data/article_cats_201708.tsv.gz'\n",
    "loader.table_data_labels = '../data/gt_data/table_pair_evaluation_eq_sub_irrel_labels.tsv'\n",
    "loader.out_dir = base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dadb85c01add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnode2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword2vec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnode2vec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnode2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "# load first the embeddings\n",
    "word2vec = None\n",
    "node2vec = None\n",
    "if word2vec is None and node2vec is None:\n",
    "    loader.load_embeddings()\n",
    "    word2vec = loader.word2vec\n",
    "    node2vec = loader.node2vec\n",
    "else:\n",
    "    loader.word2vec = word2vec\n",
    "    loader.node2vec = node2vec\n",
    "'Loaded word embeddings with %d entries and node2vec embeddings with %d entries' % (len(loader.word2vec.vocab), len(loader.node2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a03e489de30a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# construct the vocabularies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m'Loaded the word and entity vocabularies'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "# construct the vocabularies\n",
    "loader.construct_vocab()\n",
    "'Loaded the word and entity vocabularies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f81911729301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# entity_cats = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentity_cats\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_cats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_entity_cats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mentity_cats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_cats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "# entity_cats = None\n",
    "if entity_cats is None or len(entity_cats) == 0:\n",
    "    loader.load_entity_cats()\n",
    "    entity_cats = loader.entity_cats\n",
    "else:\n",
    "    loader.entity_cats = entity_cats\n",
    "'Loaded the entity categories for %d entities' % (len(loader.entity_cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-703491c8dd3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcat_tax\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_tax\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_flat_cat_tax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcat_tax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat_tax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcat_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat_level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "if cat_tax is None or len(cat_tax) == 0:\n",
    "    loader.load_flat_cat_tax()\n",
    "    cat_tax = loader.cat_tax\n",
    "    cat_level = loader.cat_level\n",
    "else:\n",
    "    loader.cat_tax = cat_tax\n",
    "    loader.cat_level = cat_level\n",
    "'Loaded the category taxonomy with %d entries' % (len(loader.cat_tax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-801ead9d74e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_alignment_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_alignment_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m'Loaded the table data with %d entries'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "tables = None\n",
    "if tables is not None:\n",
    "    loader.tables = tables\n",
    "    loader.load_alignment_pairs()\n",
    "else:\n",
    "    loader.load_alignment_pairs()\n",
    "    tables = loader.table\n",
    "'Loaded the table data with %d entries' % (len(loader.tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loader.construct_eval_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8a65dac117d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_evaluation_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m'Constructed the evaluation data for all tables'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "loader.load_evaluation_data()\n",
    "'Constructed the evaluation data for all tables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-48764be5bd52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create the matrices for the embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0memb_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word2vec_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0memb_n2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_node2vec_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "# create the matrices for the embeddings\n",
    "emb_w2v = loader.get_word2vec_matrix(256)\n",
    "emb_n2v = loader.get_node2vec_matrix(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1f52213316b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'col_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mX_subj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'col_subject'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "X_subj = []\n",
    "X_val = []\n",
    "Y = []\n",
    "\n",
    "for inst in loader.eval_pairs:\n",
    "    X += inst['col_name']\n",
    "    X_subj += inst['col_subject']\n",
    "    X_val += inst['col_values']\n",
    "    Y += [inst['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelBinarizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-87b764e0d8c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LabelBinarizer' is not defined"
     ]
    }
   ],
   "source": [
    "encoder = LabelBinarizer()\n",
    "Y_fit = encoder.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-dc5ae4757de8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-909b25a5ab2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_subj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "LEN = max(set([len(x) for x in X]) | set([len(x) for x in X_val]) | set([len(x) for x in X_subj]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d110f53ead84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_w2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'UNK'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_val_subj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_subj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_n2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'UNK'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_val_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_n2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'UNK'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "X_val = pad_sequences(X, maxlen=LEN,value=loader.vocab_w2v['UNK'], padding='pre')\n",
    "X_val_subj = pad_sequences(X_subj, maxlen=LEN,value=loader.vocab_n2v['UNK'], padding='pre')\n",
    "X_val_val = pad_sequences(X_val, maxlen=LEN,value=loader.vocab_n2v['UNK'], padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a0870d767c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train_subj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_subj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_subj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_subj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_subj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_val, Y_fit, test_size=0.3, random_state=1)\n",
    "X_train_subj, X_test_subj, y_train_subj, y_test_subj = train_test_split(X_val_subj, Y_fit, test_size=0.3, random_state=1)\n",
    "X_train_val, X_test_val, y_train_val, y_test_val = train_test_split(X_val_val, Y_fit, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TableNet Model -- Column Title Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tablenet_desc = build_bilstm_col_model(len(emb_w2v), 256, emb_w2v, 50, LEN)\n",
    "tablenet_desc.fit(x=X_train, y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tablenet_desc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-6c15544064b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_tablenet_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtablenet_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tablenet_desc' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_tablenet_val = tablenet_desc.predict([X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b60b389eab39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmat_tablenet_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_tablenet_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    }
   ],
   "source": [
    "mat_tablenet_val = cm(y_test.argmax(axis=1), y_pred_tablenet_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_metrics_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-11e88e229adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_metrics_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_tablenet_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TableNet - Column'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_metrics_report' is not defined"
     ]
    }
   ],
   "source": [
    "print(evaluation_metrics_report(mat_tablenet_val, encoder.classes_, 'TableNet - Column', 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TableNet Model - Column LCA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a1402b5bb0fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtablenet_desc_lca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_bilstm_col_subject_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_w2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_n2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_n2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "tablenet_desc_lca = build_bilstm_col_subject_model(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tablenet_desc_lca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fa6d3e05e1e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtablenet_desc_lca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_subj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tablenet_desc_lca' is not defined"
     ]
    }
   ],
   "source": [
    "tablenet_desc_lca.fit(x=[X_train, X_train_subj], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tablenet_desc_lca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e3422e1972f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_tablenet_desc_lca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtablenet_desc_lca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_subj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tablenet_desc_lca' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_tablenet_desc_lca = tablenet_desc_lca.predict([X_test, X_test_subj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ea472455636b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmat_tablenet_desc_lca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_tablenet_desc_lca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    }
   ],
   "source": [
    "mat_tablenet_desc_lca = cm(y_test.argmax(axis=1), y_pred_tablenet_desc_lca.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_metrics_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f6c92205f031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_metrics_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_tablenet_desc_lca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TableNet - Column+LCA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_metrics_report' is not defined"
     ]
    }
   ],
   "source": [
    "print(evaluation_metrics_report(mat_tablenet_desc_lca, encoder.classes_, 'TableNet - Column+LCA', 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TableNet Model - Column title, LCA and Value representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-83035dfd3a49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtablenet_desc_val_lca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_bilstm_col_subject_val_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_w2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_n2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_n2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "tablenet_desc_val_lca = build_bilstm_col_subject_val_model(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablenet_desc_val_lca.fit(x=[X_train, X_train_subj, X_train_val], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tablenet_col_lca_val = tablenet_desc_val_lca.predict([X_test, X_test_subj, X_test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tablenet_col_lca_val = cm(y_test.argmax(axis=1), y_pred_tablenet_col_lca_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_metrics_report(m_tablenet_col_lca_val, encoder.classes_, 'TableNet - Column+VAL+LCA', 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TableNet Model - Column title, and value representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablenet_desc_val = build_bilstm_col_subject_model(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablenet_desc_val.fit(x=[X_train, X_train_val], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tablenet_col_val = tablenet_desc_val.predict([X_test, X_test_val])\n",
    "m4 = cm(y_test.argmax(axis=1), y_pred_tablenet_col_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_metrics_report(m4, encoder.classes_, 'TableNet - Column+VAL', 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM baseline - column title, LCA, value representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_baseline_lca = build_bilstm_baseline_w2v_lca_val(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_baseline_lca.fit(x=[X_train, X_train_subj, X_train_val], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_blst_lca_val = bilstm_baseline_lca.predict([X_test, X_test_subj, X_test_val])\n",
    "blstm_lca_scores = cm(y_test.argmax(axis=1), y_pred_blst_lca_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_metrics_report(blstm_lca_scores, encoder.classes_, 'BiLSTM - Column+VAL+LCA', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM baseline - column title, VAL representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_baseline_w2v_val = build_bilstm_baseline_w2v_lca(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_baseline_w2v_val.fit(x=[X_train, X_train_val], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_b_w2v_val = bilstm_baseline_w2v_val.predict([X_test, X_test_subj])\n",
    "b_w2v_val_mat = cm(y_test.argmax(axis=1), y_pred_b_w2v_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_metrics_report(b_w2v_val_mat, encoder.classes_, 'BiLSTM - Column+VAL', 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM baseline - column title, LCA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_baseline_w2v_lca = build_bilstm_baseline_w2v_lca(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_baseline_w2v_lca.fit(x=[X_train, X_train_subj], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_b_w2v_lca = bilstm_baseline_w2v_lca.predict([X_test, X_test_subj])\n",
    "b_w2v_lca_mat = cm(y_test.argmax(axis=1), y_pred_b_w2v_lca.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_metrics_report(b_w2v_lca_mat, encoder.classes_, 'BiLSTM - Column+LCA', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM baseline - column title representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_baseline_w2v = build_bilstm_baseline_w2v(len(emb_w2v), 256, emb_w2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_baseline_w2v.fit(x=X_train, y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_b_w2v = bilstm_baseline_w2v.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_w2v_mat = cm(y_test.argmax(axis=1), y_pred_b_w2v.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_metrics_report(b_w2v_mat, encoder.classes_, 'BiLSTM - Column', 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM baseline - column title, and VAL representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_w2v_val = build_lstm_baseline_w2v_lca(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_w2v_val.fit(x=[X_train, X_train_val], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lstm_w2v_val = model_lstm_w2v_val.predict([X_test, X_test_subj])\n",
    "lstm_w2v_val_mat = cm(y_test.argmax(axis=1), y_pred_lstm_w2v_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_metrics_report(lstm_w2v_val_mat, encoder.classes_, 'LSTM - Column + VAL', 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM baseline - column title, and LCA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_w2v = build_lstm_baseline_w2v_lca(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)\n",
    "model_lstm_w2v.fit(x=[X_train, X_train_subj], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lstm_w2v_lca = model_lstm_w2v.predict([X_test, X_test_subj])\n",
    "lstm_w2v_lca_mat = cm(y_test.argmax(axis=1), y_pred_lstm_w2v_lca.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_metrics_report(lstm_w2v_lca_mat, encoder.classes_, 'LSTM - Column + LCA', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM baseline  - column title representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_w2v_col = build_lstm_baseline_w2v(len(emb_w2v), 256, emb_w2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_w2v_col.fit(x=X_train, y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lstm_w2v_col = model_lstm_w2v_col.predict(X_test)\n",
    "lstm_w2v_col_mat = cm(y_test.argmax(axis=1), y_pred_lstm_w2v_col.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_metrics_report(lstm_w2v_col_mat, encoder.classes_, 'LSTM - Column', 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM baseline - column title, LCA, value representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_w2v_lca_val = build_lstm_baseline_w2v_lca_val(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_w2v_lca_val.fit(x=[X_train, X_train_subj, X_train_val], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lstm_w2v_col_lca_val = model_lstm_w2v_lca_val.predict([X_test, X_test_subj, X_test_val])\n",
    "lstm_w2v_col_lca_val_mat = cm(y_test.argmax(axis=1), y_pred_lstm_w2v_col_lca_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_metrics_report(lstm_w2v_col_lca_val_mat, encoder.classes_, 'LSTM - Column+VAL+LCA', 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
